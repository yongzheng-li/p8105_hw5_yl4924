---
title: "p8105_hw5_yl4924"
author: "Yongzheng Li"
date: "2022-11-16"
output: github_document
---
```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)
library(purrr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```



# Problem 2

### Load and clean the data
```{r}
url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data <- read_csv(url) %>% 
  janitor::clean_names() %>% 
  mutate(
    state = case_when(state == "AL" & city == "Tulsa" ~ "OK",
                      TRUE                            ~ state)) %>%
  unite("city_state", c(city, state), sep = ", ", remove = FALSE)
```
The raw data-homicide_data included homicide cases over the past decade in 50 of the largest American cities. The dataset contain `r nrow(homicide_data)` rows and `r ncol(homicide_data)` columns. the important variable include reproted_date, victim_race, victim_age, victim_sex, city, state, and disposition. When cleaning data, there is one rows has city in Tulsa but state in AL. I changed this row from "AL" to "OK" to make the dataset consistent.

### summarize table of total number of homicide and totoal number of unsolved homicide
```{r}
homicide_city_count <- 
  homicide_data %>% 
  mutate(
    unsolved = disposition %in% c("Open/No arrest", "Closed without arrest")
  ) %>%  
  group_by(city_state) %>% 
  summarize(total_n = n(), 
            unsolved_n = sum(disposition %in% c("Open/No arrest", "Closed without arrest")))
head(homicide_city_count)
```

### prop test for city of Baltimore, MD
```{r}
homicide_city_count %>% 
  filter(city_state == "Baltimore, MD") %>%
  mutate(
  output = map2(.x = unsolved_n, .y = total_n, ~prop.test(.x, .y))
  )
```


```{r}
balti_count <- homicide_city_count %>% 
  filter(city_state == "Baltimore, MD")
res <- prop.test(
    x = balti_count %>% pull(unsolved_n),
    n = balti_count %>% pull(total_n)) %>% 
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```


### prop test for each city
```{r}
test <- homicide_city_count %>% 
  mutate(
    prop_test = map2(.x = unsolved_n, .y = total_n, ~prop.test(.x, .y)),
    prop_test = map(prop_test, broom::tidy)) %>%
  unnest(prop_test) %>% 
  select(city_state, unsolved_n, total_n, estimate, conf.low, conf.high)
```


### plot for estimate and CI for each city
```{r}
test %>% 
  mutate(
    city_state = forcats::fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


# Problem 3

### set up simulation function
```{r}
sim_function <- function(n_obs = 30, sigma = 5,  mu) {
  sim_data = tibble(
  x = list(rnorm(n = n_obs, mean = mu, sd = sigma)))
  return(sim_data)
}
```

### generate 5000 datasets from the model follow N(n = 30, mu=0, sigma = 5)
```{r}
output = tibble()
for (i in 1:5000) {
  output = bind_rows(output, sim_function(mu = 0))
}


```

### Save mu_hat and p-value from t-test
```{r}
output %>% 
    mutate(
      res = map(x, t.test),
      res = map(res, broom::tidy)) %>% 
  unnest(res) %>% 
  select(estimate, p.value)
```


### function for different mu value
```{r}
sim_result <- function(true_mu) {
  output = tibble()
  for (i in 1:5000) {
    output = bind_rows(output, sim_function(mu = true_mu))
}
  output = 
    output %>% 
    mutate(
      true_value = true_mu,
      res = map(x, t.test),
      res = map(res, broom::tidy)) %>% 
  unnest(res) %>% 
  select(true_value, estimate, p.value)
  return(output)
}
```

### apply function for mu from 1 to 6
```{r}
set.seed(1)
sim_res <- tibble()
for (i in 1:6) {
  sim_res = bind_rows(sim_res, sim_result(i))
}
```


### plot1

```{r}
sim_res %>% 
  group_by(true_value) %>% 
  summarize(reject = sum(p.value < 0.05)) %>% 
  ggplot(aes(x = true_value, y = reject/5000)) +
  geom_point() +
  labs(y = "Power")
```

From the plot, as the effect increase, power increase but smaller rate.

### Plot 2.1
```{r}
sim_res %>% 
  group_by(true_value) %>% 
  summarize(mean_estimate = mean(estimate)) %>% 
  ggplot(aes(x = true_value, y = mean_estimate)) +
  geom_point()
```

### Plot 2.2
```{r}
sim_res %>% 
  filter(p.value < 0.05) %>% 
  group_by(true_value) %>% 
  summarize(mean_estimate_rejected = mean(estimate)) %>% 
  ggplot(aes(x = true_value, y = mean_estimate_rejected)) +
  geom_line() +
  geom_point()
```

From the the plot, the sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true value of $\mu$, this can be explained by CLT, the estimated mean$\sim N(\mu,\frac{\sigma^2}{n})$






